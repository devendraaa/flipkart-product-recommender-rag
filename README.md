ğŸ›’ Flipkart Product Recommender (RAG-based AI System)

An end-to-end Retrieval-Augmented Generation (RAG) based product recommendation system for Flipkart-style e-commerce data.
The system leverages LangChain, Groq LLM, and AstraDB Vector Store to provide context-aware, history-aware, and accurate product recommendations, deployed using Docker and Kubernetes with production-grade monitoring.

ğŸš€ Problem Statement

E-commerce platforms like Flipkart host millions of products and reviews.
Traditional keyword-based search often fails to deliver context-aware, personalized, and conversational recommendations.

This project addresses that gap by building a RAG-powered AI recommender that:

understands user intent,

retrieves relevant product knowledge,

and generates human-like, helpful recommendations using LLMs.

ğŸ§  Solution Overview

This system combines vector-based retrieval with LLM reasoning to recommend products intelligently.

High-level flow:

User submits a query (e.g., â€œBest headphones under â‚¹2000 with good bassâ€)

Query is rewritten using history-aware retriever

Relevant product data is fetched from AstraDB Vector Store

Retrieved context is injected into Groq LLM

LLM generates concise, creative, and grounded recommendations

ğŸ—ï¸ Architecture

User
  â†“
Flask Web UI
  â†“
LangChain RAG Pipeline
  â”œâ”€â”€ Query Rephraser (History-aware)
  â”œâ”€â”€ AstraDB Vector Store (Embeddings)
  â”œâ”€â”€ Context Retrieval
  â””â”€â”€ Groq LLM (Response Generation)
  â†“
Dockerized Application
  â†“
Kubernetes (GCP)
  â†“
Prometheus (Monitoring & Metrics)

ğŸ› ï¸ Tech Stack
AI / LLM

Groq LLM â€“ ultra-fast inference

LangChain â€“ RAG orchestration

HuggingFace Embeddings â€“ vector generation

Data & Retrieval

AstraDB Vector Store â€“ scalable vector search

History-aware Retriever â€“ context preservation

Backend & Deployment

Flask â€“ web application

Docker â€“ containerization

Kubernetes (Minikube / GCP) â€“ orchestration

kubectl â€“ cluster management

Observability

Prometheus â€“ application monitoring

Custom metrics â€“ request count, latency

Version Control

GitHub

âœ¨ Key Features

ğŸ” RAG-based product recommendations

ğŸ§  History-aware conversational retrieval

âœï¸ Creative, LLM-generated product summaries

âš¡ Low-latency responses using Groq

ğŸ³ Fully containerized with Docker

â˜¸ï¸ Kubernetes-based scalable deployment

ğŸ“Š Production monitoring using Prometheus

ğŸ”„ Modular, extensible architecture

ğŸ“Œ Example Use Cases

â€œBest Bluetooth headphones under â‚¹3000â€

â€œCompare these with Sony modelsâ€

â€œWhich one has better battery life?â€

â€œIs it good for gaming?â€

The system remembers conversation context and responds accordingly.

ğŸ“ˆ Production Readiness

This project is designed with real-world deployment considerations:

Stateless application containers

Vector database for scalable retrieval

Kubernetes orchestration

Monitoring for performance and reliability

Clean separation of ingestion, retrieval, and generation layers

ğŸ§  What I Learned

Designing and implementing RAG pipelines for real-world data

Building history-aware conversational AI systems

Integrating LLMs with vector databases

Deploying AI systems using Docker & Kubernetes

Monitoring AI applications using Prometheus

Debugging real-world deployment issues (image pull errors, cluster configs)


Conversation history is preserved for follow-up questions

ğŸ”® Future Improvements

Add real-time data ingestion (scraping / APIs)

Implement RAG evaluation (RAGAS)

Add Grafana dashboards

Improve UI/UX

Multi-model routing for cost optimization

ğŸ‘¤ Author

Devendra Umesh Chavan
AI Engineer
Founder â€“ Saavo Avinya Pvt Ltd

â­ Why This Project Matters

This project demonstrates:

LLM engineering

RAG system design

Cloud-native deployment

Production observability

End-to-end AI application thinking

It reflects how modern AI systems are built and deployed in industry, not just demos.
